{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ›¡ï¸ Mosaic Protocol - Vulnerability Classifier V3\n",
                "\n",
                "## Golden Test Set Strategy\n",
                "- **TRAIN** on 31K noisy V1 data (Kaggle)\n",
                "- **TEST** on 145 expert-labeled SmartBugs (truth)\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "import os\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/mosaic-ml'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "!pip install xgboost scikit-learn imbalanced-learn matplotlib seaborn --quiet\n",
                "print(\"âœ… Ready\")"
            ],
            "metadata": {
                "id": "setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import json, gzip, numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "# Load hybrid dataset (train on noisy, test on golden)\n",
                "DATA_DIR = '/content/drive/MyDrive/mosaic-ml'\n",
                "data_files = sorted(Path(DATA_DIR).glob('hybrid_dataset_*.json*'))\n",
                "if not data_files:\n",
                "    raise FileNotFoundError(f\"Upload hybrid_dataset_*.json.gz to {DATA_DIR}\")\n",
                "\n",
                "data_file = data_files[-1]\n",
                "print(f\"ðŸ“‚ Loading: {data_file.name}\")\n",
                "\n",
                "with gzip.open(data_file, 'rt', encoding='utf-8') if str(data_file).endswith('.gz') else open(data_file) as f:\n",
                "    data = json.load(f)\n",
                "\n",
                "print(f\"\"\"\\nðŸ“Š Dataset:\n",
                "   Train: {data['metadata']['trainSamples']:,} (noisy)\n",
                "   Golden Test: {data['metadata']['testSamples']} (expert)\n",
                "   Features: {data['metadata']['featureCount']}\n",
                "\"\"\")"
            ],
            "metadata": {
                "id": "load_data"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "def to_arrays(samples):\n",
                "    X = np.array([s['features'] for s in samples], dtype=np.float32)\n",
                "    y = np.array([s['label'] for s in samples], dtype=np.int32)\n",
                "    return X, y\n",
                "\n",
                "X_train, y_train = to_arrays(data['train'])\n",
                "X_val, y_val = to_arrays(data['validation'])\n",
                "X_golden, y_golden = to_arrays(data['goldenTest'])\n",
                "\n",
                "print(f\"Train: {X_train.shape} (safe:{(y_train==0).sum()}, vuln:{(y_train==1).sum()})\")\n",
                "print(f\"Val: {X_val.shape}\")\n",
                "print(f\"Golden Test: {X_golden.shape} (safe:{(y_golden==0).sum()}, vuln:{(y_golden==1).sum()})\")\n",
                "\n",
                "feature_names = data['metadata']['featureNames']"
            ],
            "metadata": {
                "id": "arrays"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Apply SMOTE to training data\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "print(f\"Before SMOTE: safe={(y_train==0).sum()}, vuln={(y_train==1).sum()}\")\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
                "print(f\"After SMOTE: safe={(y_train_balanced==0).sum()}, vuln={(y_train_balanced==1).sum()}\")"
            ],
            "metadata": {
                "id": "smote"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from xgboost import XGBClassifier\n",
                "\n",
                "model = XGBClassifier(\n",
                "    n_estimators=150,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.08,\n",
                "    eval_metric='aucpr',\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "print(\"ðŸš€ Training on 31K noisy data...\")\n",
                "model.fit(X_train_balanced, y_train_balanced, eval_set=[(X_val, y_val)], verbose=30)\n",
                "print(\"âœ… Training complete!\")"
            ],
            "metadata": {
                "id": "train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ðŸŽ¯ Golden Test Evaluation (Truth)"
            ],
            "metadata": {
                "id": "eval_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Evaluate on GOLDEN test (expert labels)\n",
                "y_golden_pred = model.predict(X_golden)\n",
                "y_golden_proba = model.predict_proba(X_golden)[:, 1]\n",
                "\n",
                "print(\"ðŸŽ¯ GOLDEN TEST EVALUATION (Expert Labels):\")\n",
                "print(\"=\"*50)\n",
                "print(classification_report(y_golden, y_golden_pred, target_names=['Safe', 'Vulnerable']))\n",
                "\n",
                "if len(np.unique(y_golden)) > 1:\n",
                "    roc_auc = roc_auc_score(y_golden, y_golden_proba)\n",
                "    print(f\"\\nðŸŽ¯ ROC-AUC on Golden Set: {roc_auc:.4f}\")"
            ],
            "metadata": {
                "id": "golden_eval"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Confusion Matrix on Golden\n",
                "plt.figure(figsize=(8, 6))\n",
                "cm = confusion_matrix(y_golden, y_golden_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
                "            xticklabels=['Safe', 'Vuln'],\n",
                "            yticklabels=['Safe', 'Vuln'])\n",
                "plt.title('Golden Test Confusion Matrix')\n",
                "plt.ylabel('Actual (Expert)')\n",
                "plt.xlabel('Predicted')\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{OUTPUT_DIR}/golden_confusion_matrix.png\")\n",
                "plt.show()"
            ],
            "metadata": {
                "id": "golden_cm"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# PR Curve on Golden\n",
                "precision, recall, thresholds = precision_recall_curve(y_golden, y_golden_proba)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(recall, precision, 'g-', linewidth=2)\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "plt.title('Precision-Recall on Golden Expert Set')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig(f\"{OUTPUT_DIR}/golden_pr_curve.png\")\n",
                "plt.show()\n",
                "\n",
                "# Find optimal threshold\n",
                "target_recall = 0.85\n",
                "idx = np.where(recall >= target_recall)[0]\n",
                "if len(idx) > 0:\n",
                "    best_idx = idx[-1]\n",
                "    optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
                "    print(f\"\\nðŸŽ¯ For {target_recall:.0%} recall: threshold={optimal_threshold:.3f}, precision={precision[best_idx]:.3f}\")\n",
                "else:\n",
                "    optimal_threshold = 0.3\n",
                "    best_idx = 0"
            ],
            "metadata": {
                "id": "golden_pr"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Feature Importance\n",
                "importances = model.feature_importances_\n",
                "indices = np.argsort(importances)[::-1][:15]\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.bar(range(15), importances[indices])\n",
                "plt.xticks(range(15), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
                "plt.title('Top 15 Features (Hybrid Model)')\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{OUTPUT_DIR}/feature_importance_v3.png\")\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ” Top 10 Features:\")\n",
                "for i, idx in enumerate(indices[:10]):\n",
                "    print(f\"   {i+1}. {feature_names[idx]}: {importances[idx]:.4f}\")"
            ],
            "metadata": {
                "id": "features"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import joblib\n",
                "\n",
                "# Save model\n",
                "model_path = f\"{OUTPUT_DIR}/vulnerability_classifier_v3.pkl\"\n",
                "joblib.dump(model, model_path)\n",
                "print(f\"âœ… Saved: {model_path}\")\n",
                "\n",
                "# Save metadata\n",
                "metadata = {\n",
                "    'model_name': 'vulnerability_classifier_v3',\n",
                "    'strategy': 'golden_test_set',\n",
                "    'created_at': str(np.datetime64('now')),\n",
                "    'feature_names': feature_names,\n",
                "    'optimal_threshold': float(optimal_threshold),\n",
                "    'golden_metrics': {\n",
                "        'roc_auc': float(roc_auc) if 'roc_auc' in dir() else 0,\n",
                "        'recall_at_threshold': float(recall[best_idx]) if len(idx) > 0 else 0,\n",
                "        'precision_at_threshold': float(precision[best_idx]) if len(idx) > 0 else 0,\n",
                "    },\n",
                "    'train_samples': int(len(y_train_balanced)),\n",
                "    'golden_test_samples': int(len(y_golden)),\n",
                "}\n",
                "\n",
                "with open(f\"{OUTPUT_DIR}/model_metadata_v3.json\", 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "\n",
                "print(\"\\nðŸ“‹ Metadata saved. Key metrics:\")\n",
                "print(f\"   Threshold: {optimal_threshold:.3f}\")\n",
                "print(f\"   Golden ROC-AUC: {metadata['golden_metrics']['roc_auc']:.4f}\")"
            ],
            "metadata": {
                "id": "save"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## âœ… Complete!\n",
                "\n",
                "**Key insight:** The Golden Test metrics are your TRUE accuracy.\n",
                "\n",
                "If Golden PR curve looks good (not diagonal), your model learned real patterns despite noisy training data."
            ],
            "metadata": {
                "id": "done"
            }
        }
    ]
}