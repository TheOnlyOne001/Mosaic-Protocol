{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üõ°Ô∏è Mosaic Protocol - Smart Contract Vulnerability Classifier\n",
                "\n",
                "This notebook trains an XGBoost classifier to detect vulnerable smart contracts.\n",
                "\n",
                "**Dataset:** 37,000+ labeled contracts (safe vs exploit)\n",
                "**Model:** XGBoost with class imbalance handling\n",
                "**Output:** ONNX model for Node.js integration\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1Ô∏è‚É£ Setup & Mount Google Drive"
            ],
            "metadata": {
                "id": "setup"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mount_drive"
            },
            "outputs": [],
            "source": [
                "# Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create output directory\n",
                "import os\n",
                "OUTPUT_DIR = '/content/drive/MyDrive/mosaic-ml'\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"‚úÖ Output directory: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Install dependencies (including onnxmltools for XGBoost ONNX export)\n",
                "!pip install xgboost scikit-learn onnxmltools onnx onnxruntime matplotlib seaborn --quiet\n",
                "print(\"‚úÖ Dependencies installed\")"
            ],
            "metadata": {
                "id": "install_deps"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2Ô∏è‚É£ Load Training Data"
            ],
            "metadata": {
                "id": "load_data"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "import gzip\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "\n",
                "# Find the training data file\n",
                "DATA_DIR = '/content/drive/MyDrive/mosaic-ml'\n",
                "\n",
                "# Look for .json.gz or .json files\n",
                "data_files = list(Path(DATA_DIR).glob('training_data_*.json*'))\n",
                "if not data_files:\n",
                "    raise FileNotFoundError(f\"No training data found in {DATA_DIR}. Please upload training_data_*.json.gz\")\n",
                "\n",
                "data_file = sorted(data_files)[-1]  # Use most recent\n",
                "print(f\"üìÇ Loading: {data_file.name}\")\n",
                "\n",
                "# Load data (handle both gzip and plain JSON)\n",
                "if str(data_file).endswith('.gz'):\n",
                "    with gzip.open(data_file, 'rt', encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "else:\n",
                "    with open(data_file, 'r') as f:\n",
                "        data = json.load(f)\n",
                "\n",
                "# Display metadata\n",
                "print(f\"\"\"\\nüìä Dataset Summary:\n",
                "   Total samples: {data['metadata']['totalSamples']:,}\n",
                "   Features: {data['metadata']['featureCount']}\n",
                "   Safe: {data['metadata']['labelDistribution']['safe']:,}\n",
                "   Exploit: {data['metadata']['labelDistribution']['exploit']:,}\n",
                "\"\"\")"
            ],
            "metadata": {
                "id": "load_json"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Convert to numpy arrays\n",
                "def to_arrays(samples):\n",
                "    X = np.array([s['features'] for s in samples], dtype=np.float32)\n",
                "    y = np.array([s['label'] for s in samples], dtype=np.int32)\n",
                "    return X, y\n",
                "\n",
                "X_train, y_train = to_arrays(data['train'])\n",
                "X_val, y_val = to_arrays(data['validation'])\n",
                "X_test, y_test = to_arrays(data['test'])\n",
                "\n",
                "print(f\"\"\"üìê Array shapes:\n",
                "   Train: {X_train.shape} ({(y_train==0).sum()} safe, {(y_train==1).sum()} exploit)\n",
                "   Val:   {X_val.shape} ({(y_val==0).sum()} safe, {(y_val==1).sum()} exploit)\n",
                "   Test:  {X_test.shape} ({(y_test==0).sum()} safe, {(y_test==1).sum()} exploit)\n",
                "\"\"\")\n",
                "\n",
                "# Feature names\n",
                "feature_names = data['metadata']['featureNames']\n",
                "print(f\"Feature names ({len(feature_names)}): {feature_names[:5]}...\")"
            ],
            "metadata": {
                "id": "convert_arrays"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3Ô∏è‚É£ Train XGBoost Model"
            ],
            "metadata": {
                "id": "train"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from xgboost import XGBClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, roc_auc_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Calculate class weight for imbalance handling\n",
                "# Higher weight on exploit class to reduce false negatives\n",
                "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
                "print(f\"‚öñÔ∏è Class imbalance ratio: {scale_pos_weight:.2f}:1 (safe:exploit)\")\n",
                "print(f\"   Using scale_pos_weight={scale_pos_weight:.2f} to boost exploit detection\")\n",
                "\n",
                "# Create and train model\n",
                "model = XGBClassifier(\n",
                "    n_estimators=200,\n",
                "    max_depth=6,\n",
                "    learning_rate=0.1,\n",
                "    scale_pos_weight=scale_pos_weight,  # Handle class imbalance\n",
                "    max_delta_step=1,  # Stabilize with imbalanced data\n",
                "    eval_metric='aucpr',  # Precision-Recall AUC\n",
                "    use_label_encoder=False,\n",
                "    random_state=42,\n",
                "    verbosity=1\n",
                ")\n",
                "\n",
                "print(\"\\nüöÄ Training XGBoost classifier...\")\n",
                "model.fit(\n",
                "    X_train, y_train,\n",
                "    eval_set=[(X_val, y_val)],\n",
                "    verbose=20  # Print every 20 iterations\n",
                ")\n",
                "print(\"‚úÖ Training complete!\")"
            ],
            "metadata": {
                "id": "train_xgb"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4Ô∏è‚É£ Evaluate Model"
            ],
            "metadata": {
                "id": "evaluate"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Get predictions\n",
                "y_pred = model.predict(X_test)\n",
                "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
                "\n",
                "# Classification report\n",
                "print(\"üìä Classification Report (default threshold=0.5):\")\n",
                "print(classification_report(y_test, y_pred, target_names=['Safe', 'Exploit']))\n",
                "\n",
                "# ROC-AUC\n",
                "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "print(f\"üéØ ROC-AUC Score: {roc_auc:.4f}\")"
            ],
            "metadata": {
                "id": "eval_basic"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Confusion Matrix\n",
                "plt.figure(figsize=(8, 6))\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=['Safe', 'Exploit'],\n",
                "            yticklabels=['Safe', 'Exploit'])\n",
                "plt.title('Confusion Matrix')\n",
                "plt.ylabel('Actual')\n",
                "plt.xlabel('Predicted')\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{OUTPUT_DIR}/confusion_matrix.png\")\n",
                "plt.show()\n",
                "print(f\"üíæ Saved: {OUTPUT_DIR}/confusion_matrix.png\")"
            ],
            "metadata": {
                "id": "confusion_matrix"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5Ô∏è‚É£ Optimize Threshold for High Recall"
            ],
            "metadata": {
                "id": "threshold"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# For security classifiers, we want HIGH RECALL on exploits\n",
                "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
                "\n",
                "# Find threshold for 95% recall\n",
                "target_recall = 0.95\n",
                "idx = np.where(recall >= target_recall)[0]\n",
                "if len(idx) > 0:\n",
                "    best_idx = idx[-1]\n",
                "    optimal_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
                "    print(f\"üéØ For {target_recall:.0%} recall on exploits:\")\n",
                "    print(f\"   Optimal threshold: {optimal_threshold:.3f}\")\n",
                "    print(f\"   Precision at this threshold: {precision[best_idx]:.3f}\")\n",
                "else:\n",
                "    optimal_threshold = 0.3\n",
                "    best_idx = 0\n",
                "    print(f\"‚ö†Ô∏è Using default threshold: {optimal_threshold}\")\n",
                "\n",
                "# Apply optimized threshold\n",
                "y_pred_optimized = (y_pred_proba >= optimal_threshold).astype(int)\n",
                "print(\"\\nüìä Classification Report (optimized threshold):\")\n",
                "print(classification_report(y_test, y_pred_optimized, target_names=['Safe', 'Exploit']))"
            ],
            "metadata": {
                "id": "optimize_threshold"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Plot Precision-Recall curve\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(recall, precision, 'b-', linewidth=2)\n",
                "plt.axvline(x=target_recall, color='r', linestyle='--', label=f'Target recall ({target_recall:.0%})')\n",
                "plt.xlabel('Recall (Sensitivity)')\n",
                "plt.ylabel('Precision')\n",
                "plt.title('Precision-Recall Curve for Exploit Detection')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{OUTPUT_DIR}/precision_recall_curve.png\")\n",
                "plt.show()"
            ],
            "metadata": {
                "id": "pr_curve"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6Ô∏è‚É£ Feature Importance"
            ],
            "metadata": {
                "id": "feature_importance"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Plot feature importance\n",
                "importances = model.feature_importances_\n",
                "indices = np.argsort(importances)[::-1][:15]\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.bar(range(15), importances[indices])\n",
                "plt.xticks(range(15), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
                "plt.title('Top 15 Most Important Features')\n",
                "plt.tight_layout()\n",
                "plt.savefig(f\"{OUTPUT_DIR}/feature_importance.png\")\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüîù Top 10 Features:\")\n",
                "for i, idx_feat in enumerate(indices[:10]):\n",
                "    print(f\"   {i+1}. {feature_names[idx_feat]}: {importances[idx_feat]:.4f}\")"
            ],
            "metadata": {
                "id": "feature_imp"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 7Ô∏è‚É£ Export Model to ONNX"
            ],
            "metadata": {
                "id": "export"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Use onnxmltools for XGBoost (sklearn-onnx doesn't support XGBoost directly)\n",
                "from onnxmltools import convert_xgboost\n",
                "from onnxmltools.convert.common.data_types import FloatTensorType\n",
                "import onnx\n",
                "\n",
                "# Define input type\n",
                "num_features = X_train.shape[1]\n",
                "initial_type = [('features', FloatTensorType([None, num_features]))]\n",
                "\n",
                "# Convert XGBoost to ONNX\n",
                "print(\"üîÑ Converting XGBoost model to ONNX format...\")\n",
                "onnx_model = convert_xgboost(\n",
                "    model,\n",
                "    initial_types=initial_type,\n",
                "    target_opset=12\n",
                ")\n",
                "\n",
                "# Save ONNX model\n",
                "onnx_path = f\"{OUTPUT_DIR}/vulnerability_classifier.onnx\"\n",
                "onnx.save_model(onnx_model, onnx_path)\n",
                "\n",
                "onnx_size = os.path.getsize(onnx_path) / 1024 / 1024\n",
                "print(f\"‚úÖ Saved: {onnx_path} ({onnx_size:.2f} MB)\")"
            ],
            "metadata": {
                "id": "export_onnx"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Verify ONNX model works\n",
                "import onnxruntime as ort\n",
                "\n",
                "print(\"üîç Verifying ONNX model...\")\n",
                "session = ort.InferenceSession(onnx_path)\n",
                "\n",
                "# Get input name from model\n",
                "input_name = session.get_inputs()[0].name\n",
                "print(f\"   Input name: {input_name}\")\n",
                "\n",
                "# Test prediction\n",
                "test_input = X_test[:5]\n",
                "onnx_pred = session.run(None, {input_name: test_input})\n",
                "\n",
                "print(f\"   Input shape: {test_input.shape}\")\n",
                "print(f\"   Output predictions: {onnx_pred[0][:5]}\")\n",
                "print(f\"   Expected labels: {y_test[:5]}\")\n",
                "print(\"‚úÖ ONNX model verification passed!\")"
            ],
            "metadata": {
                "id": "verify_onnx"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 8Ô∏è‚É£ Save Model Metadata"
            ],
            "metadata": {
                "id": "metadata"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Save model metadata for Node.js integration\n",
                "model_metadata = {\n",
                "    'model_name': 'vulnerability_classifier',\n",
                "    'model_type': 'XGBoost',\n",
                "    'created_at': str(np.datetime64('now')),\n",
                "    'num_features': int(num_features),\n",
                "    'feature_names': feature_names,\n",
                "    'classes': ['safe', 'exploit'],\n",
                "    'optimal_threshold': float(optimal_threshold),\n",
                "    'input_name': input_name,\n",
                "    'metrics': {\n",
                "        'roc_auc': float(roc_auc),\n",
                "        'accuracy': float((y_pred == y_test).mean()),\n",
                "        'recall_exploit': float(recall[best_idx]) if len(idx) > 0 else 0,\n",
                "        'precision_exploit': float(precision[best_idx]) if len(idx) > 0 else 0,\n",
                "    },\n",
                "    'training_samples': int(len(y_train)),\n",
                "    'class_distribution': {\n",
                "        'safe': int((y_train == 0).sum()),\n",
                "        'exploit': int((y_train == 1).sum()),\n",
                "    }\n",
                "}\n",
                "\n",
                "metadata_path = f\"{OUTPUT_DIR}/model_metadata.json\"\n",
                "with open(metadata_path, 'w') as f:\n",
                "    json.dump(model_metadata, f, indent=2)\n",
                "\n",
                "print(f\"‚úÖ Saved: {metadata_path}\")\n",
                "print(\"\\nüìã Model Metadata:\")\n",
                "for k, v in model_metadata.items():\n",
                "    if k not in ['feature_names']:\n",
                "        print(f\"   {k}: {v}\")"
            ],
            "metadata": {
                "id": "save_metadata"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ‚úÖ Training Complete!\n",
                "\n",
                "**Files saved to Google Drive:**\n",
                "1. `vulnerability_classifier.onnx` - The trained model\n",
                "2. `model_metadata.json` - Model configuration and metrics\n",
                "3. `confusion_matrix.png` - Evaluation visualization\n",
                "4. `precision_recall_curve.png` - Threshold analysis\n",
                "5. `feature_importance.png` - Feature ranking\n",
                "\n",
                "**Next Steps:**\n",
                "1. Download `vulnerability_classifier.onnx` and `model_metadata.json`\n",
                "2. Place in `backend/src/agents/defi-safety/ml/models/`\n",
                "3. The inference service will load and use the model"
            ],
            "metadata": {
                "id": "complete"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# List all output files\n",
                "print(\"\\nüìÅ Output files in Google Drive:\")\n",
                "for f in os.listdir(OUTPUT_DIR):\n",
                "    size = os.path.getsize(f\"{OUTPUT_DIR}/{f}\") / 1024\n",
                "    print(f\"   {f} ({size:.1f} KB)\")"
            ],
            "metadata": {
                "id": "list_files"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}